{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "authorized-numbers",
   "metadata": {},
   "source": [
    "Step 1. 데이터 수집하기\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acquired-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "portuguese-million",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mathematical-boating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19285</th>\n",
       "      <td>2 dead, over 1,500 test positive for dengue in...</td>\n",
       "      <td>Two people died due to dengue and 1,588 others...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36841</th>\n",
       "      <td>Didn't know about money paid to pornstar: Trump</td>\n",
       "      <td>US President Donald Trump gave his statement o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85939</th>\n",
       "      <td>India's 1st Grand Slam was a French Open mixed...</td>\n",
       "      <td>India's first ever Grand Slam victory was Mahe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66739</th>\n",
       "      <td>Himachal Ranji player scores triple century on...</td>\n",
       "      <td>Himachal Pradesh Ranji batsman Prashant Chopra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75410</th>\n",
       "      <td>What things are most sought after as dowry in ...</td>\n",
       "      <td>Dowry harassment allegations in Delhi almost d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16229</th>\n",
       "      <td>Sachin all-time favorite, admire Virat a lot: ...</td>\n",
       "      <td>Australia's all-time leading goal-scorer Tim C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56585</th>\n",
       "      <td>Man City post 4-0 win, extend record win strea...</td>\n",
       "      <td>Premier League leaders Manchester City extende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50419</th>\n",
       "      <td>Trailer of Urvashi Rautela starrer 'Hate Story...</td>\n",
       "      <td>The trailer of Urvashi Rautela starrer 'Hate S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56510</th>\n",
       "      <td>1st tied Test was caused by direct hit off pen...</td>\n",
       "      <td>Australia needed one run off two balls with on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23683</th>\n",
       "      <td>7 of a family found dead in Jharkhand, 2 of th...</td>\n",
       "      <td>Seven members of a family including two infant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "19285  2 dead, over 1,500 test positive for dengue in...   \n",
       "36841    Didn't know about money paid to pornstar: Trump   \n",
       "85939  India's 1st Grand Slam was a French Open mixed...   \n",
       "66739  Himachal Ranji player scores triple century on...   \n",
       "75410  What things are most sought after as dowry in ...   \n",
       "16229  Sachin all-time favorite, admire Virat a lot: ...   \n",
       "56585  Man City post 4-0 win, extend record win strea...   \n",
       "50419  Trailer of Urvashi Rautela starrer 'Hate Story...   \n",
       "56510  1st tied Test was caused by direct hit off pen...   \n",
       "23683  7 of a family found dead in Jharkhand, 2 of th...   \n",
       "\n",
       "                                                    text  \n",
       "19285  Two people died due to dengue and 1,588 others...  \n",
       "36841  US President Donald Trump gave his statement o...  \n",
       "85939  India's first ever Grand Slam victory was Mahe...  \n",
       "66739  Himachal Pradesh Ranji batsman Prashant Chopra...  \n",
       "75410  Dowry harassment allegations in Delhi almost d...  \n",
       "16229  Australia's all-time leading goal-scorer Tim C...  \n",
       "56585  Premier League leaders Manchester City extende...  \n",
       "50419  The trailer of Urvashi Rautela starrer 'Hate S...  \n",
       "56510  Australia needed one run off two balls with on...  \n",
       "23683  Seven members of a family including two infant...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-archives",
   "metadata": {},
   "source": [
    "Step 2. 데이터 전처리하기 (추상적 요약)\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deluxe-corruption",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "synthetic-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metric-addiction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "transsexual-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ordinary-roberts",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426.53434896469116  seconds\n",
      "['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers'\n",
      " 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit'\n",
      " 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india getting seventh lowest total odi cricket history'\n",
      " ...\n",
      " 'according reports new version science fiction film matrix development michael jordan reportedly play lead role film screenwriter zak penn talks write script film reports added actor keanu reeves starred original film followed two sequels'\n",
      " 'new music video shows rapper snoop dogg aiming toy gun clown character parodying us president donald trump video also shows tv airing news conference headline ronald klump wants deport doggs airing live clown house video remixed version song lavender'\n",
      " 'madhesi morcha alliance seven political parties withdrawn support pm pushpa kamal dahal led nepal government failed meet seven day ultimatum fulfil demands including endorsement revised constitution amendment bill morcha seats parliament despite withdrawal support immediate threat government']\n",
      "83.29105019569397  seconds\n",
      "['upgrad learner switches career ml al salary hike'\n",
      " 'delhi techie wins free food swiggy one year cred'\n",
      " 'new zealand end rohit sharma led india match winning streak' ...\n",
      " 'matrix film get reboot reports'\n",
      " 'snoop dogg aims gun clown dressed trump new video'\n",
      " 'madhesi morcha withdraws support nepalese government']\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp   # 멀티 프로세싱으로 전처리 속도를 획기적으로 줄여봅시다\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial  # map을 할 때 함수에 여러 인자를 넣어줄 수 있도록 합니다\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# num_cores 만큼 쪼개진 데이터를 전처리하여 반환합니다\n",
    "def appendTexts(sentences, remove_stopwords):\n",
    "  texts = []\n",
    "  for s in sentences:\n",
    "    texts += preprocess_sentence(s, remove_stopwords),\n",
    "  return texts\n",
    "\n",
    "def preprocess_data(data, remove_stopwords=True):\n",
    "  start_time = time.time()\n",
    "  num_cores = mp.cpu_count()  # 컴퓨터의 코어 수를 구합니다\n",
    "\n",
    "  text_data_split = np.array_split(data, num_cores)  # 코어 수만큼 데이터를 배분하여 병렬적으로 처리할 수 있게 합니다\n",
    "  pool = Pool(num_cores)\n",
    "\n",
    "  processed_data = np.concatenate(pool.map(partial(appendTexts, remove_stopwords=remove_stopwords), text_data_split))  # 각자 작업한 데이터를 하나로 합쳐줍니다\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  print(time.time() - start_time, \" seconds\")\n",
    "  return processed_data\n",
    "\n",
    "clean_text = preprocess_data(data['text'])  # 클라우드 기준으로 3~4분 정도 소요 됩니다\n",
    "print(clean_text)\n",
    "\n",
    "clean_headlines = preprocess_data(data['headlines']) # 클라우드 기준 1분정도 소요됩니다.\n",
    "print(clean_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impressed-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['headlines'] = clean_headlines\n",
    "data['text'] = clean_text\n",
    "\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "indoor-summer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-battle",
   "metadata": {},
   "source": [
    "- text, headlines의 길이를 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "headline_len = [len(s.split()) for s in data['headlines']]\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "\n",
    "print('text의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('text의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('text의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('headline의 최소 길이 : {}'.format(np.min(headline_len)))\n",
    "print('headline의 최대 길이 : {}'.format(np.max(headline_len)))\n",
    "print('headline의 평균 길이 : {}'.format(np.mean(headline_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(headline_len)\n",
    "plt.title('headlines')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('headlines')\n",
    "plt.hist(headline_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "burning-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 40\n",
    "headline_max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "republican-middle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acting-middle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 40 이하인 샘플의 비율: 0.9238320748772878\n",
      "전체 샘플 중 길이가 10 이하인 샘플의 비율: 0.9978353878517495\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(headline_max_len,  data['headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "equipped-headquarters",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 90772\n"
     ]
    }
   ],
   "source": [
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headline_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "understanding-employee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upgrad learner switches career ml al salary hike</td>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>sostoken upgrad learner switches career ml al ...</td>\n",
       "      <td>upgrad learner switches career ml al salary hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>known hirani yrs metoo claims true sonam</td>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>sostoken known hirani yrs metoo claims true sonam</td>\n",
       "      <td>known hirani yrs metoo claims true sonam eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rahat fateh ali khan denies getting notice smu...</td>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>sostoken rahat fateh ali khan denies getting n...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice smu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0   upgrad learner switches career ml al salary hike   \n",
       "2  new zealand end rohit sharma led india match w...   \n",
       "3  aegon life iterm insurance plan helps customer...   \n",
       "4           known hirani yrs metoo claims true sonam   \n",
       "5  rahat fateh ali khan denies getting notice smu...   \n",
       "\n",
       "                                                text  \\\n",
       "0  saurav kant alumnus upgrad iiit pg program mac...   \n",
       "2  new zealand defeated india wickets fourth odi ...   \n",
       "3  aegon life iterm insurance plan customers enjo...   \n",
       "4  speaking sexual harassment allegations rajkuma...   \n",
       "5  pakistani singer rahat fateh ali khan denied r...   \n",
       "\n",
       "                                       decoder_input  \\\n",
       "0  sostoken upgrad learner switches career ml al ...   \n",
       "2  sostoken new zealand end rohit sharma led indi...   \n",
       "3  sostoken aegon life iterm insurance plan helps...   \n",
       "4  sostoken known hirani yrs metoo claims true sonam   \n",
       "5  sostoken rahat fateh ali khan denies getting n...   \n",
       "\n",
       "                                      decoder_target  \n",
       "0  upgrad learner switches career ml al salary hi...  \n",
       "2  new zealand end rohit sharma led india match w...  \n",
       "3  aegon life iterm insurance plan helps customer...  \n",
       "4  known hirani yrs metoo claims true sonam eostoken  \n",
       "5  rahat fateh ali khan denies getting notice smu...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tough-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = np.array(data['text']) # 인코더의 입력 \n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력 \n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블 \n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "played-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79808 85040 41902 ... 27791 12452 82781]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aggregate-touch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "neutral-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 18154\n"
     ]
    }
   ],
   "source": [
    "# 8:2의 비율로 데이터 분리\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "supreme-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 72618\n",
      "훈련 레이블의 개수 : 72618\n",
      "테스트 데이터의 개수 : 18154\n",
      "테스트 레이블의 개수 : 18154\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "executive-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "talented-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 65708\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 44708\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 21000\n",
      "단어 집합에서 희귀 단어의 비율: 68.04042125768551\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.643306570977176\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stopped-adolescent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_vocab = 20000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 20,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "specific-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2643, 1670, 2823, 1858, 1199, 3247, 240, 749, 894, 527, 4, 43, 3909, 87, 1858, 1670, 3535, 4818, 180, 1670, 206, 145, 2058, 1148, 1287, 793, 384, 8618, 1287, 109, 607, 27, 9110, 14544, 109], [35, 593, 16, 202, 139, 75, 128, 101, 6631, 61, 121, 54, 235, 155, 110, 785, 4351, 2287, 388, 27, 220, 3395, 6713, 139, 84, 926, 3086, 5226, 17414, 95, 186, 15, 3169, 3136, 347, 128, 515, 161, 17414], [298, 1194, 927, 3681, 2787, 3992, 11150, 69, 61, 121, 54, 1, 448, 7378, 21, 19598, 4, 18408, 684, 427, 2124, 21, 17, 109, 913, 2524, 10119, 2524, 139, 4, 559, 1327, 974, 1063, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "affiliated-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# headlines에도 적용\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nominated-concrete",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 28720\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 18899\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 9821\n",
      "단어 집합에서 희귀 단어의 비율: 65.80431754874651\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 6.161905588839686\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "opposed-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 682, 945, 1758, 8946, 677, 386], [1, 47, 185, 5016, 7, 1559, 49, 282, 2211], [1, 1344, 9823, 3, 7, 21], [1, 2482, 1937, 7, 6705, 1249], [1, 1154, 1759, 4152, 6045, 3905, 66, 153, 169]]\n",
      "target\n",
      "decoder  [[682, 945, 1758, 8946, 677, 386, 2], [47, 185, 5016, 7, 1559, 49, 282, 2211, 2], [1344, 9823, 3, 7, 21, 2], [2482, 1937, 7, 6705, 1249, 2], [1154, 1759, 4152, 6045, 3905, 66, 153, 169, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 등장빈도가 낮은 단어들 제거\n",
    "tar_vocab = 10000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "guided-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 2\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 72616\n",
      "훈련 레이블의 개수 : 72616\n",
      "테스트 데이터의 개수 : 18154\n",
      "테스트 레이블의 개수 : 18154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-venue",
   "metadata": {},
   "source": [
    "- 패딩하기<br/> 서로다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "selected-terror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='pre')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='pre')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=headline_max_len, padding='pre')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=headline_max_len, padding='pre')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=headline_max_len, padding='pre')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=headline_max_len, padding='pre')\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-breeding",
   "metadata": {},
   "source": [
    "- 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "apparent-killing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "danish-suggestion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "normal-seating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 40, 128)      2560000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 40, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 40, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    1280000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 40, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10000)  2570000     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,249,104\n",
      "Trainable params: 8,249,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-queen",
   "metadata": {},
   "source": [
    "Step 3. 어텐션 메커니즘 사용하기 (추상적 요약)\n",
    "------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-patch",
   "metadata": {},
   "source": [
    "- 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "collaborative-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "advised-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 40, 128)      2560000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 40, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 40, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    1280000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 40, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 10000)  5130000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,940,432\n",
      "Trainable params: 10,940,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "played-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "284/284 [==============================] - 210s 698ms/step - loss: 5.9170 - val_loss: 5.2145\n",
      "Epoch 2/50\n",
      "284/284 [==============================] - 196s 690ms/step - loss: 5.2573 - val_loss: 4.9518\n",
      "Epoch 3/50\n",
      "284/284 [==============================] - 198s 696ms/step - loss: 4.9273 - val_loss: 4.9080\n",
      "Epoch 4/50\n",
      "284/284 [==============================] - 197s 694ms/step - loss: 4.6576 - val_loss: 4.4838\n",
      "Epoch 5/50\n",
      "284/284 [==============================] - 197s 694ms/step - loss: 4.4278 - val_loss: 4.3221\n",
      "Epoch 6/50\n",
      "284/284 [==============================] - 197s 694ms/step - loss: 4.2265 - val_loss: 4.1852\n",
      "Epoch 7/50\n",
      "284/284 [==============================] - 197s 692ms/step - loss: 4.0544 - val_loss: 4.0782\n",
      "Epoch 8/50\n",
      "284/284 [==============================] - 198s 696ms/step - loss: 3.9087 - val_loss: 3.9982\n",
      "Epoch 9/50\n",
      "284/284 [==============================] - 197s 693ms/step - loss: 3.7723 - val_loss: 3.9295\n",
      "Epoch 10/50\n",
      "284/284 [==============================] - 197s 695ms/step - loss: 3.6491 - val_loss: 3.9177\n",
      "Epoch 11/50\n",
      "284/284 [==============================] - 196s 690ms/step - loss: 3.5356 - val_loss: 3.8147\n",
      "Epoch 12/50\n",
      "284/284 [==============================] - 212s 747ms/step - loss: 3.4384 - val_loss: 3.7914\n",
      "Epoch 13/50\n",
      "284/284 [==============================] - 198s 696ms/step - loss: 3.3484 - val_loss: 3.7390\n",
      "Epoch 14/50\n",
      "284/284 [==============================] - 196s 690ms/step - loss: 3.2655 - val_loss: 3.7125\n",
      "Epoch 15/50\n",
      "284/284 [==============================] - 196s 691ms/step - loss: 3.1889 - val_loss: 3.6864\n",
      "Epoch 16/50\n",
      "284/284 [==============================] - 198s 696ms/step - loss: 3.1159 - val_loss: 3.6644\n",
      "Epoch 17/50\n",
      "284/284 [==============================] - 197s 693ms/step - loss: 3.0516 - val_loss: 3.6456\n",
      "Epoch 18/50\n",
      "284/284 [==============================] - 196s 692ms/step - loss: 2.9869 - val_loss: 3.6340\n",
      "Epoch 19/50\n",
      "284/284 [==============================] - 196s 691ms/step - loss: 2.9206 - val_loss: 3.6223\n",
      "Epoch 20/50\n",
      "284/284 [==============================] - 197s 695ms/step - loss: 2.8645 - val_loss: 3.6033\n",
      "Epoch 21/50\n",
      "284/284 [==============================] - 197s 694ms/step - loss: 2.8083 - val_loss: 3.6017\n",
      "Epoch 22/50\n",
      "284/284 [==============================] - 196s 691ms/step - loss: 2.7638 - val_loss: 3.5898\n",
      "Epoch 23/50\n",
      "284/284 [==============================] - 196s 691ms/step - loss: 2.7105 - val_loss: 3.5850\n",
      "Epoch 24/50\n",
      "284/284 [==============================] - 197s 695ms/step - loss: 2.6753 - val_loss: 3.5743\n",
      "Epoch 25/50\n",
      "284/284 [==============================] - 197s 694ms/step - loss: 2.6299 - val_loss: 3.5721\n",
      "Epoch 26/50\n",
      "284/284 [==============================] - 197s 693ms/step - loss: 2.5890 - val_loss: 3.5697\n",
      "Epoch 27/50\n",
      "284/284 [==============================] - 196s 691ms/step - loss: 2.5416 - val_loss: 3.5669\n",
      "Epoch 28/50\n",
      "284/284 [==============================] - 197s 695ms/step - loss: 2.5054 - val_loss: 3.5615\n",
      "Epoch 29/50\n",
      "284/284 [==============================] - 197s 693ms/step - loss: 2.4745 - val_loss: 3.5593\n",
      "Epoch 30/50\n",
      "284/284 [==============================] - 196s 691ms/step - loss: 2.4424 - val_loss: 3.5663\n",
      "Epoch 31/50\n",
      "284/284 [==============================] - 196s 691ms/step - loss: 2.4031 - val_loss: 3.5736\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-chester",
   "metadata": {},
   "source": [
    "- 훈련 데이터의 손실과 검증 데이터의 손실이 줄어드는 과정을 시각화 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "young-marina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "patent-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "subsequent-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "controversial-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "regional-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (headline_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-bidder",
   "metadata": {},
   "source": [
    "Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "duplicate-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "further-dream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : bjp mlas rajasthan thursday chanted jai shri ram house celebrate landslide victory party recently held uttar pradesh assembly polls started rajasthan minister rajendra rathore asked opposition congratulate party victory led verbal altercation bjp mlas opposition mlas \n",
      "실제 요약 : bjp mlas chant jai shri ram raj assembly win \n",
      "예측 요약 :  bjp mla takes oath bjp chief th consecutive term\n",
      "\n",
      "\n",
      "원문 : banks india closed nearly atms may february according rbi data february total number atms banks come however number atms saw slight increase addition atms months \n",
      "실제 요약 : banks close nearly atms months \n",
      "예측 요약 :  rbi may lose gdp last year old currency sbi\n",
      "\n",
      "\n",
      "원문 : asylum seekers morocco nigeria china elsewhere illegally tried enter europe using world cup fan identity documents gives visa free access russia according border police accused intercepted russia border finland norway border poland notably border belarus russia crossed without checks \n",
      "실제 요약 : asylum seekers use world cup ids enter europe russia \n",
      "예측 요약 :  china jails women rights china amid china war report\n",
      "\n",
      "\n",
      "원문 : government friday issued thunderstorm warnings west bengal odisha bihar uttar pradesh people killed dust storms met department officials predicted another dust storm parts rajasthan uttar pradesh next hours department also issued five day warning thunderstorms heavy rainfall across nation \n",
      "실제 요약 : govt issues thunderstorm warning dust storm kills \n",
      "예측 요약 :  andhra govt announces lakh boat cyclone ockhi kills andhra\n",
      "\n",
      "\n",
      "원문 : chef sanjeev kapoor revealed visit uae pm narendra modi served flat bread nine ancient grains breakfast dal rice always featured plate kapoor said dinner abu dhabi saffron rice infused stock different indian spices cake served \n",
      "실제 요약 : pm narendra modi eat visit uae \n",
      "예측 요약 :  pm modi pic pm modi travel travel travel pic\n",
      "\n",
      "\n",
      "원문 : steve smith received one test ban admitted australia deliberately conspired cheat third test south africa cameron bancroft caught rubbing ball yellow tape coach darren lehmann talked th man allegedly told bancroft hide bancroft seen putting material trousers \n",
      "실제 요약 : watch incident steve smith received test ban \n",
      "예측 요약 :  smith warner gets ball tampering ball tampering aus pacer\n",
      "\n",
      "\n",
      "원문 : china decided temporarily stop funding least three major road projects built part china pakistan economic corridor pakistan following reports corruption officials said funds would released china issues new guidelines decision could affect pakistan road projects worth crore \n",
      "실제 요약 : china stops funding cpec projects pak fraud \n",
      "예측 요약 :  china plans build china road china amid standoff report\n",
      "\n",
      "\n",
      "원문 : earning crore third day ranbir kapoor sanju beat record set hindi version baahubali collect highest single day earnings hindi film baahubali collected crore third day sanju biopic sanjay dutt entered crore club within three days release \n",
      "실제 요약 : sanju beats baahubali collects highest single day earnings \n",
      "예측 요약 :  sanju beats hindi hindi ranbir sanju enter cr report\n",
      "\n",
      "\n",
      "원문 : indian cricket team opener rohit sharma paid tribute stan lee creator marvel comics passed away monday age marvel movie complete without thank leaving us greatest heroes time giving favourite superhero hulk rip rohit wrote \n",
      "실제 요약 : rohit pays tribute marvel stan lee thanks hulk \n",
      "예측 요약 :  rohit sharma dedicates madrid star wars series vs rohit\n",
      "\n",
      "\n",
      "원문 : union government monday launched online film certification system central board film certification make process films obtain certificate fully digital include qr codes check fraudulent certificates highlight online film certification elimination middlemen corruption said minister venkaiah naidu \n",
      "실제 요약 : govt launches online film system \n",
      "예측 요약 :  cbfc govt proposes hindi medium ek prem katha app\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-caution",
   "metadata": {},
   "source": [
    "Step 5. Summa을 이용해서 추출적 요약해보기\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "prospective-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "discrete-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n'.join(data['text'][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "significant-stevens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "['government officials wednesday said swine flu outbreak killed people year rajasthan colder weather worsened seasonal outbreak virus country officials added rajasthan recorded third swine flu deaths india jodhpur worst affected area state officials said', 'following hardik pandya return suspension comments women koffee karan india captain virat kohli said controversy help year old rounder scale new heights career good headspace hope goes right path emerges better cricketer think added kohli', 'talking year old batsman shubman gill called india squad new zealand series india captain virat kohli said exciting talent saw bat nets like wow even kohli added shubman averages first class cricket', 'rjd leader tejashwi yadav said congress president rahul gandhi qualities required become good prime minister national president india oldest party parliament last years added forget party five chief ministers country leading added', 'talking priyanka gandhi entering active politics samajwadi party president akhilesh yadav saturday said would like congratulate congress party president took right decision young people given chance samajwadi party happy added priyanka appointed congress general secretary charge eastern uttar pradesh wednesday']\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "touched-vermont",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "following hardik pandya return suspension comments women koffee karan india captain virat kohli said controversy help year old rounder scale new heights career good headspace hope goes right path emerges better cricketer think added kohli\n",
      "talking year old batsman shubman gill called india squad new zealand series india captain virat kohli said exciting talent saw bat nets like wow even kohli added shubman averages first class cricket\n",
      "rjd leader tejashwi yadav said congress president rahul gandhi qualities required become good prime minister national president india oldest party parliament last years added forget party five chief ministers country leading added\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, words=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-delta",
   "metadata": {},
   "source": [
    "회고\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-lesbian",
   "metadata": {},
   "source": [
    "- text와 headlines의 max 값을 정했을때 비율이 1.0 0.9 뭐 다양하게 나왔는데<br/> 수치를 계속 변경해 보려 했으나 샘플 수가 이전 값에 정지하여 변하지 않는 것 때문에 restart를 했고...멀티 프로세싱은 너무나 오래걸리고...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-delight",
   "metadata": {},
   "source": [
    "- 결과적으로 Summa를 활용한 추출적 요약이... 정확도도 높고 좋다하는데 음...<br/> 결과를 보면 그렇게 와닿진 않았던 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-scope",
   "metadata": {},
   "source": [
    "- 모델 훈련이 너무 걸렸다 1 epoch당 210초...? 엄청나다..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
